{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLW2bvjLHzzG",
        "colab_type": "text"
      },
      "source": [
        "**Text Generation using Bidirectional LSTM**\n",
        "\n",
        "A simple implementation of text generation using a deep LSTM NN, inspired by a tutorial I found at [link](https://www.machinelearningmastery.com)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beN573SR7YbE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, LSTM, Bidirectional\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils\n",
        "\n",
        "# Load input file\n",
        "input = \"/content/drive/My Drive/LSTM/TheOneWithSeason5.txt\"\n",
        "raw = open(input, 'r', encoding='utf-8').read()\n",
        "raw = raw.lower()\n",
        "\n",
        "# Map\n",
        "chars = sorted(list(set(raw)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uokBQ_AOI-1J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Code for Data Loading and Displaying results taken from: https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\n",
        "\n",
        "# Summarize\n",
        "n_chars = len(raw)\n",
        "n_vocab = len(chars)\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "\tseq_in = raw[i:i + seq_length]\n",
        "\tseq_out = raw[i + seq_length]\n",
        "\tdataX.append([char_to_int[char] for char in seq_in])\n",
        "\tdataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)\n",
        "\n",
        "print(\"Total Characters: \", n_chars)\n",
        "print(\"Total Vocab: \", n_vocab)\n",
        "print(\"Total Patterns: \", n_patterns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3p04ZCDJBnP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define Model\n",
        "model = Sequential()\n",
        "model.add(Bidirectional(LSTM(128, activation=\"relu\"),input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfES-_mkJH_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rroZBVcF-xDQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "\n",
        "## Put file Name for Weights for model here \n",
        "filename = \"/content/weights-improvement-20-2.4410.hdf5\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "\n",
        "print(\"Seed:\")\n",
        "print(\"\", ''.join([int_to_char[value] for value in pattern]))\n",
        "\n",
        "for i in range(1000):\n",
        "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(n_vocab)\n",
        "\tprediction = model.predict(x, verbose=0)\n",
        "\tindex = numpy.argmax(prediction)\n",
        "\tresult = int_to_char[index]\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\n",
        "\tsys.stdout.write(result)\n",
        "\tpattern.append(index)\n",
        "\tpattern = pattern[1:len(pattern)]\n",
        "print \"\\nDone.\""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}